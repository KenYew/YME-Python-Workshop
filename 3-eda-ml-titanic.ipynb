{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://ymeglobal.org/wp-content/uploads/2019/02/YME-LOGO-2017.png\" width=\"300px\" align = \"left\"/>\n",
    "</div>\n",
    "\n",
    "# YME: Data Science with Machine Learning Workshop\n",
    "## Exploratory Data Analysis and Prediction with Machine Learning\n",
    "\n",
    "### Case Study: Titanic Survival Analysis and Prediction\n",
    "\n",
    "The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew.  This sensational tragedy shocked the international community and led to better safety regulations for ships.\n",
    "\n",
    "One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.  Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n",
    "\n",
    "##### In this notebook, we will complete the analysis of what sorts of people were likely to survive. In particular, we will apply the tools of machine learning to predict which passengers survived the tragedy.\n",
    "\n",
    "#### Required Libraries:\n",
    "* [NumPy](http://www.numpy.org/)\n",
    "* [Pandas](http://pandas.pydata.org/)\n",
    "* [Matplotlib](http://matplotlib.org/)\n",
    "* [Seaborn](https://seaborn.pydata.org/index.html)\n",
    "* [SciKit-Learn](http://scikit-learn.org/stable/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Author: Ken Yew Piong\n",
    "\n",
    "<i class=\"fa fa-linkedin-square fa-1x\" aria-hidden=\"true\"></i> Linkedin: [**@Ken Yew Piong**](https://www.linkedin.com/in/ken-yew-piong/)\n",
    "\n",
    "<i class=\"fa fa-github-square fa-1x\" aria-hidden=\"true\"></i> GitHub: [**@KenYew**](https://github.com/KenYew)\n",
    "\n",
    "<i class=\"fa fa-facebook-square fa-1x\" aria-hidden=\"true\"></i> Messenger: [**@kkenyew**](https://m.me/kkenyew)\n",
    "\n",
    "<i class=\"fa fa-envelope-square\" aria-hidden=\"true\"></i> Mail: josephpiong@live.com\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Table of Contents\n",
    "\n",
    "* **Intro:** Data Science and Machine Learning Project Workflow\n",
    "* **Chapter 1:** Quick Overview of the Data\n",
    "* **Chapter 2:** Data Wrangling and Analysis with Pandas and Seaborn\n",
    "* **Chapter 3:** Data Visualisation with Matplotlib\n",
    "* **Chapter 4:** Feature Engineering with Pandas\n",
    "* **Chapter 5:** Machine Learning with Scikit-Learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data Science and Machine Learning Project Workflow\n",
    "\n",
    "<!-- \n",
    "<div>\n",
    "<img src=\"https://image.slidesharecdn.com/applieddatasciencecourse21-181003133730/95/applied-data-science-course-part-2-the-data-science-workflow-and-basic-models-deep-dive-5-1024.jpg?cb=1538573971\" width=\"700px\" align = \"middle\"/>\n",
    "</div> -->\n",
    "\n",
    "<div>\n",
    "<img src=\"https://image.slidesharecdn.com/datanativespres-161031143514/95/machine-learning-and-internet-of-things-the-future-of-medical-prevention-4-1024.jpg?cb=1477924745\" width=\"800px\" align = \"middle\"/>\n",
    "</div>\n",
    "\n",
    "## Step 1: Understanding the Problem and Business Objectives \n",
    "> - Define the objective in business terms\n",
    "> - How should you frame this problem? (using Supervised/Unsupervised machine learning, online/offline, etc.)\n",
    "\n",
    "## Step 2: Data Acquisition \n",
    "> - List the data you need and how much you need\n",
    "> - Find and document where you can get that data\n",
    "> - Get the data and convert the data to a format you can easily manipulate. \n",
    "> - Sample a test dataset and put it aside! Never look at it until machine learning!\n",
    "\n",
    "## Step 3: Data Wrangling and Analysis \n",
    "> - Study each attribute and its characteristics (e.g.: Classes (categorical), age (continuous))\n",
    "> - Identify extra data that would be useful\n",
    "    \n",
    "## Step 4: Data Visualization \n",
    "> - Visualise the data and study correlations between attributes\n",
    "\n",
    "## Step 5: Data Cleaning\n",
    "> - Fix or remove outliers\n",
    "> - Fill in missing values (e.g.: with zero, mean, median) or drop their rows (or columns)\n",
    "\n",
    "## Step 6: Feature Engineering \n",
    "> - Discretize continuous features\n",
    "> - Decompose features (e.g.: categorical, date/time, etc.)\n",
    "> - Add promising transformations of features (e.g.: log(x), sqrt(x), x^2, etc.)\n",
    "> - Aggregate features into promising new features\n",
    "> - Standardize or normalize features\n",
    "\n",
    "## Step 7: Model training, experimentation and evaluation \n",
    "> - Train many quick and dirty models from different categories (e.g.: linear, logistic, SVM, Random Forests, etc.) using standard parameters\n",
    "> - Measure and compare their accuracy performance\n",
    "> - Short-list the top three to five most promising models\n",
    "> - Fine-tune the hyperparameters using cross-validation\n",
    "> - Try different approaches such as ensemble methods and deep learning if needed\n",
    "> - Measure and compare accuracy performances on test dataset\n",
    "\n",
    "## Step 8: Present the Solution and Launch the Product \n",
    "> - Document what you have done\n",
    "> - Create a nice presentation highlighting the big picture first before going into how your solution achieves the business objectives\n",
    "> - Ensure key findings are communicated through beautiful visualizations and memorable statements\n",
    "> - Launch your product!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data analysis and wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Chapter 1: Quick Overview of the Data\n",
    "## 1.1 Data Acquisition\n",
    "\n",
    "```tex\n",
    "Let's start by reading our data using Pandas! \n",
    "Please ensure that the folder dataset is in the same directory as this Jupyter notebook file.\n",
    "```\n",
    "\n",
    "#### __Dataset Import__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./dataset/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# The Pandas Library\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/KenYew/YME-Python-Workshop/master/images/pandas_logo.png\" width=\"500px\" align = \"left\"/>\n",
    "</div>\n",
    "\n",
    "Pandas is an open source library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language. It is fully compatible with the NumPy package.\n",
    "\n",
    "Instead of axis Pandas uses \"index\" and \"column\" to describe the dimensions\n",
    "\n",
    "<!-- <div>\n",
    "<img src=\"images/pandas_basic.png\" width=\"500px\" align = \"left\"/>\n",
    "</div> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1.2 Data Overview\n",
    "### __Step 1: Quick Glance of the DataFrame__\n",
    "```python\n",
    "The pd.head() method allows us to observe the first 5 rows of the data from the dataframe.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "The pd.tail() method allows us to observe the last 5 rows of the data from the dataframe.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Or simply print the whole dataframe to see everything.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick reference of feature abbreviations\n",
    "|Feature|Definition|Key|\n",
    "|:--------- |:----------------|:------------- |\n",
    "|**survival**|Survival|0 = No, 1 = Yes|\n",
    "|**pclass**|Ticket class|1 = 1st, 2 = 2nd, 3 = 3rd|\n",
    "|**sex**|Sex|\n",
    "|**Age**|Age in years|\n",
    "|**sibsp**|# of siblings / spouses aboard the Titanic|\n",
    "|**parch**|# of parents / children aboard the Titanic|\n",
    "|**ticket**|Ticket number|\n",
    "|**fare**|Passenger fare|\n",
    "|**cabin**|Cabin number|\n",
    "|**embarked**|Port of Embarkation|C = Cherbourg, Q = Queenstown, S = Southampton|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Indexing and Selecting Data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Fare']\n",
    "# df['Fare'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0]['Fare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0, 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### __Step 2: Analyse the features of the dataset__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```tex\n",
    "We see that the features that describe each passenger are separated into 4 types of data: \n",
    "1. Categorical: Survived, Sex, and Embarked\n",
    "2. Ordinal (rank): Pclass (Passenger Class)\n",
    "3. Continuous: Age, Fare\n",
    "4. Discrete: SibSp, Parch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### __Step 3: Dataset Statistics__\n",
    "\n",
    "#### __1. Use pd.info() for data overview__\n",
    "\n",
    "```python\n",
    "The pd.info() method is an extremely useful method that allows to have a quick overview of all the data types and the number of entries for each feature.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### __2. Use pd.describe() for data statistics__\n",
    "\n",
    "```python\n",
    "The pd.desribe() method provides a quick statistical overview of the data with helpful stats such as mean, standard deviation, etc. of each feature.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "However, pd.describe() by default provides us a statistical overview of continuous data only but ignores categorical and ordinal data. You will need to include an additional parameter: pd.describe(include=['all']) to include all feature types.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Let's take a look now with our recent findings!\n",
    "\n",
    "Above is a summary of our data contained in a `Pandas` `DataFrame`. Think of a `DataFrame` as a Python's super charged version of the workflow in an Excel table. As you can see the summary holds quite a bit of information. First, it lets us know we have 891 observations, or passengers, to analyze here:\n",
    "    \n",
    "    Int64Index: 891 entries, 0 to 890\n",
    "\n",
    "Next it shows us all of the columns in `DataFrame`. Each column tells us something about each of our observations, like their `name`, `sex` or `age`. These colunms  are called a features of our dataset. You can think of the meaning of the words column and feature as interchangeable for this notebook. \n",
    "\n",
    "After each feature it lets us know how many values it contains. While most of our features have complete data on every observation, like the `survived` feature here: \n",
    "\n",
    "    survived    891  non-null values \n",
    "\n",
    "some are missing information, like the `age` feature: \n",
    "\n",
    "    age         714  non-null values \n",
    "\n",
    "These missing values are represented as `NaN`s.\n",
    "\n",
    "---\n",
    "## 1.3 Data Cleaning\n",
    "\n",
    "#### Take care of missing values!\n",
    "The features `ticket` and `cabin` have many missing values and so can’t add much value to our analysis. To handle this we will drop them from the dataframe to preserve the integrity of our dataset.\n",
    "\n",
    "To do that we'll use this line of code to drop the features entirely:\n",
    "\n",
    "```python\n",
    "    df = df.drop(['ticket','cabin'], axis=1) \n",
    "```\n",
    "\n",
    "While this line of code removes the `NaN` values from every remaining column / feature:\n",
    "```python\n",
    "    df = df.dropna()\n",
    "``` \n",
    "Now we have a clean and tidy dataset that is ready for analysis. Because `.dropna()` removes an observation from our data even if it only has 1 `NaN` in one of the features, it would have removed most of our dataset if we had not dropped the `ticket` and `cabin`  features first.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Ticket','Cabin'], axis=1) # Remove columns 'Ticket' and 'Cabin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna() # Remove NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Chapter 2: Data Wrangling and Analysis with Pandas and Seaborn\n",
    "## 2.1 Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Survivors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(df['Survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Survived'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fare Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Fare'].hist(bins = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gender of Survivors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Women and children first?\n",
    "fig, axes = plt.subplots(1,2)\n",
    "df[df['Sex'] == 'male'].Survived.value_counts().plot(kind='barh', ax = axes[0], title = 'Male Survivors')\n",
    "df[df['Sex'] == 'female'].Survived.value_counts().plot(kind='barh', ax = axes[1], title = 'Female Survivors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Age of Survivors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Age'] < 15].Survived.value_counts().plot(kind = 'barh', title = 'Survivors below the age of 15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Survived'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['Sex']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['Sex', 'Pclass']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Age'] < 18].groupby(['Sex', 'Pclass']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# The Seaborn Library\n",
    "<div>\n",
    "<img src=\"https://seabornnetworks.com/wp-content/uploads/2017/05/seaborn.jpg\" width=\"400px\" align = \"left\"/>\n",
    "</div>\n",
    "\n",
    "The Python visualization library Seaborn provides a high-level interface for drawing attractive statistical graphics. It is built on top of matplotlib and closely integrated with pandas data structures. Here is some of the functionality that seaborn offers:\n",
    "\n",
    "- Options for visualizing univariate or bivariate distributions and for comparing them between subsets of data\n",
    "- Convenient views onto the overall structure of complex datasets\n",
    "- High-level abstractions for structuring multi-plot grids that let you easily build complex visualizations\n",
    "- Concise control over matplotlib figure styling with several built-in themes\n",
    "- Tools for choosing color palettes that faithfully reveal patterns in your data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2.2 Univariate Data Analysis\n",
    "#### __Plotting count plots and distribution plots__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 4, figsize = (16, 10))\n",
    "sns.countplot('Survived', data = df, ax = ax[0,0])\n",
    "sns.countplot('Pclass', data = df, ax = ax[0,1])\n",
    "sns.countplot('Sex', data = df, ax = ax[0,2])\n",
    "sns.countplot('SibSp', data = df, ax = ax[0,3])\n",
    "sns.countplot('Parch', data = df, ax = ax[1,0])\n",
    "sns.countplot('Embarked', data = df, ax = ax[1,1])\n",
    "sns.distplot(df['Fare'], kde = True, ax = ax[1,2])\n",
    "sns.distplot(df['Age'], kde = True, ax = ax[1,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.3 Multivariate Data Analysis\n",
    "#### __Plotting the correlation matrix heatmap__\n",
    "```tex\n",
    "A correlation matrix heatmap allows us to have an overview of the correlations between each features and provide us a 'bird's eye' view of how each data column relates to another.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "plt.title('Titanic Dataset Correlation Matrix', fontsize = 24)\n",
    "\n",
    "# Compute pairwise correlation of columns\n",
    "corr = df.corr()\n",
    "\n",
    "# Colormap - maps data values to color space (where the colors denote correlation values)\n",
    "cmap = sns.diverging_palette(250, 10, s = 68, l = 40, as_cmap = True)\n",
    "\n",
    "# Plot correlation matrix to a heatmap with a custom colormap \n",
    "sns.heatmap(corr, cmap = cmap, vmin = -1, vmax = 1, annot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Matplotlib Library\n",
    "<div>\n",
    "<img src=\"https://matplotlib.org/_static/logo2_compressed.svg\" width=\"500px\" align = \"left\"/>\n",
    "</div>\n",
    "\n",
    "Matplotlib is a visualization library in Python for 2D plots of arrays. Matplotlib is a multi-platform data visualization library built on NumPy arrays. \n",
    "\n",
    "One of the greatest benefits of visualization is that it allows us visual access to huge amounts of data in easily digestible visuals. Matplotlib consists of several plots like line, bar, scatter, histogram etc.\n",
    "\n",
    "Pyplot is a Matplotlib module which provides a MATLAB-like interface, with the advantage of being free and open-source. \n",
    "\n",
    "# Chapter 3: Data Visualisation with Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# specifies the parameters of our graphs\n",
    "fig = plt.figure(figsize=(18,6), dpi=1600) \n",
    "alpha=alpha_scatterplot = 0.2 \n",
    "alpha_bar_chart = 0.55\n",
    "\n",
    "# lets us plot many diffrent shaped graphs together \n",
    "ax1 = plt.subplot2grid((2,3),(0,0))\n",
    "# plots a bar graph of those who surived vs those who did not.               \n",
    "df.Survived.value_counts().plot(kind='bar', alpha=alpha_bar_chart)\n",
    "# this nicely sets the margins in matplotlib to deal with a recent bug 1.3.1\n",
    "ax1.set_xlim(-1, 2)\n",
    "# puts a title on our graph\n",
    "plt.title(\"Distribution of Survival, (1 = Survived)\")    \n",
    "\n",
    "plt.subplot2grid((2,3),(0,1))\n",
    "plt.scatter(df.Survived, df.Age, alpha=alpha_scatterplot)\n",
    "# sets the y axis lable\n",
    "plt.ylabel(\"Age\")\n",
    "# formats the grid line style of our graphs                          \n",
    "plt.grid(b=True, which='major', axis='y')  \n",
    "plt.title(\"Survival by Age,  (1 = Survived)\")\n",
    "\n",
    "ax3 = plt.subplot2grid((2,3),(0,2))\n",
    "df.Pclass.value_counts().plot(kind=\"barh\", alpha=alpha_bar_chart)\n",
    "ax3.set_ylim(-1, len(df.Pclass.value_counts()))\n",
    "plt.title(\"Class Distribution\")\n",
    "\n",
    "plt.subplot2grid((2,3),(1,0), colspan=2)\n",
    "# plots a kernel density estimate of the subset of the 1st class passangers's age\n",
    "df.Age[df.Pclass == 1].plot(kind='kde')    \n",
    "df.Age[df.Pclass == 2].plot(kind='kde')\n",
    "df.Age[df.Pclass == 3].plot(kind='kde')\n",
    " # plots an axis lable\n",
    "plt.xlabel(\"Age\")    \n",
    "plt.title(\"Age Distribution within classes\")\n",
    "# sets our legend for our graph.\n",
    "plt.legend(('1st Class', '2nd Class','3rd Class'),loc='best') \n",
    "\n",
    "ax5 = plt.subplot2grid((2,3),(1,2))\n",
    "df.Embarked.value_counts().plot(kind='bar', alpha=alpha_bar_chart)\n",
    "ax5.set_xlim(-1, len(df.Embarked.value_counts()))\n",
    "# specifies the parameters of our graphs\n",
    "plt.title(\"Passengers per boarding location\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Visualization:\n",
    "\n",
    "The point of this competition is to predict if an individual will survive based on the features in the data like:\n",
    " \n",
    " * Traveling Class (called pclass in the data)\n",
    " * Sex \n",
    " * Age\n",
    " * Fare Price\n",
    "\n",
    "Let’s see if we can gain a better understanding of who survived and died. \n",
    "\n",
    "\n",
    "First let’s plot a bar graph of those who Survived Vs. Those who did not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "fig, ax = plt.subplots()\n",
    "df.Survived.value_counts().plot(kind='barh', color=\"blue\", alpha=.65)\n",
    "ax.set_ylim(-1, len(df.Survived.value_counts())) \n",
    "plt.title(\"Survival Breakdown (1 = Survived, 0 = Died)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let’s tease more structure out of the data,\n",
    "### Let’s break the previous graph down by gender\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,6))\n",
    "\n",
    "#create a plot of two subsets, male and female, of the survived variable.\n",
    "#After we do that we call value_counts() so it can be easily plotted as a bar graph. \n",
    "#'barh' is just a horizontal bar graph\n",
    "df_male = df.Survived[df.Sex == 'male'].value_counts().sort_index()\n",
    "df_female = df.Survived[df.Sex == 'female'].value_counts().sort_index()\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "df_male.plot(kind='barh',label='Male', alpha=0.55)\n",
    "df_female.plot(kind='barh', color='#FA2379',label='Female', alpha=0.55)\n",
    "plt.title(\"Who Survived? with respect to Gender, (raw value counts) \"); plt.legend(loc='best')\n",
    "ax1.set_ylim(-1, 2) \n",
    "\n",
    "#adjust graph to display the proportions of survival by gender\n",
    "ax2 = fig.add_subplot(122)\n",
    "(df_male/float(df_male.sum())).plot(kind='barh',label='Male', alpha=0.55)  \n",
    "(df_female/float(df_female.sum())).plot(kind='barh', color='#FA2379',label='Female', alpha=0.55)\n",
    "plt.title(\"Who Survived proportionally? with respect to Gender\"); plt.legend(loc='best')\n",
    "\n",
    "ax2.set_ylim(-1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it’s clear that although more men died and survived in raw value counts, females had a greater survival rate proportionally (~25%), than men (~20%)\n",
    "\n",
    "#### Great! But let’s go down even further:\n",
    "Can we capture more of the structure by using Pclass? Here we will bucket classes as lowest class or any of the high classes (classes 1 - 2). 3 is lowest class. Let’s break it down by Gender and what Class they were traveling in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,4), dpi=1600)\n",
    "alpha_level = 0.65\n",
    "\n",
    "# building on the previous code, here we create an additional subset with in the gender subset \n",
    "# we created for the survived variable. I know, thats a lot of subsets. After we do that we call \n",
    "# value_counts() so it it can be easily plotted as a bar graph. this is repeated for each gender \n",
    "# class pair.\n",
    "ax1=fig.add_subplot(141)\n",
    "female_highclass = df.Survived[df.Sex == 'female'][df.Pclass != 3].value_counts()\n",
    "female_highclass.plot(kind='bar', label='female, highclass', color='#FA2479', alpha=alpha_level)\n",
    "ax1.set_xticklabels([\"Survived\", \"Died\"], rotation=0)\n",
    "ax1.set_xlim(-1, len(female_highclass))\n",
    "plt.title(\"Who Survived? with respect to Gender and Class\"); plt.legend(loc='best')\n",
    "\n",
    "ax2=fig.add_subplot(142, sharey=ax1)\n",
    "female_lowclass = df.Survived[df.Sex == 'female'][df.Pclass == 3].value_counts()\n",
    "female_lowclass.plot(kind='bar', label='female, low class', color='pink', alpha=alpha_level)\n",
    "ax2.set_xticklabels([\"Died\",\"Survived\"], rotation=0)\n",
    "ax2.set_xlim(-1, len(female_lowclass))\n",
    "plt.legend(loc='best')\n",
    "\n",
    "ax3=fig.add_subplot(143, sharey=ax1)\n",
    "male_lowclass = df.Survived[df.Sex == 'male'][df.Pclass == 3].value_counts()\n",
    "male_lowclass.plot(kind='bar', label='male, low class',color='lightblue', alpha=alpha_level)\n",
    "ax3.set_xticklabels([\"Died\",\"Survived\"], rotation=0)\n",
    "ax3.set_xlim(-1, len(male_lowclass))\n",
    "plt.legend(loc='best')\n",
    "\n",
    "ax4=fig.add_subplot(144, sharey=ax1)\n",
    "male_highclass = df.Survived[df.Sex == 'male'][df.Pclass != 3].value_counts()\n",
    "male_highclass.plot(kind='bar', label='male, highclass', alpha=alpha_level, color='steelblue')\n",
    "ax4.set_xticklabels([\"Died\",\"Survived\"], rotation=0)\n",
    "ax4.set_xlim(-1, len(male_highclass))\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! Now we have a lot more information on who survived and died in the tragedy. With this deeper understanding, we are better equipped to create better more insightful models. This is a typical process in interactive data analysis. First you start small and understand the most basic relationships and slowly increment the complexity of your analysis as you discover more and more about the data you’re working with. Below is the progression of process laid out together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,12), dpi=1600)\n",
    "a = 0.65\n",
    "# Step 1\n",
    "ax1 = fig.add_subplot(341)\n",
    "df.Survived.value_counts().plot(kind='bar', color=\"blue\", alpha=a)\n",
    "ax1.set_xlim(-1, len(df.Survived.value_counts()))\n",
    "plt.title(\"Step. 1\")\n",
    "\n",
    "# Step 2\n",
    "ax2 = fig.add_subplot(345)\n",
    "df.Survived[df.Sex == 'male'].value_counts().plot(kind='bar',label='Male')\n",
    "df.Survived[df.Sex == 'female'].value_counts().plot(kind='bar', color='#FA2379',label='Female')\n",
    "ax2.set_xlim(-1, 2)\n",
    "plt.title(\"Step. 2 \\nWho Survived? with respect to Gender.\"); plt.legend(loc='best')\n",
    "\n",
    "ax3 = fig.add_subplot(346)\n",
    "(df.Survived[df.Sex == 'male'].value_counts()/float(df.Sex[df.Sex == 'male'].size)).plot(kind='bar',label='Male')\n",
    "(df.Survived[df.Sex == 'female'].value_counts()/float(df.Sex[df.Sex == 'female'].size)).plot(kind='bar', color='#FA2379',label='Female')\n",
    "ax3.set_xlim(-1,2)\n",
    "plt.title(\"Who Survied proportionally?\"); plt.legend(loc='best')\n",
    "\n",
    "\n",
    "# Step 3\n",
    "ax4 = fig.add_subplot(349)\n",
    "female_highclass = df.Survived[df.Sex == 'female'][df.Pclass != 3].value_counts()\n",
    "female_highclass.plot(kind='bar', label='female highclass', color='#FA2479', alpha=a)\n",
    "ax4.set_xticklabels([\"Survived\", \"Died\"], rotation=0)\n",
    "ax4.set_xlim(-1, len(female_highclass))\n",
    "plt.title(\"Who Survived? with respect to Gender and Class\"); plt.legend(loc='best')\n",
    "\n",
    "ax5 = fig.add_subplot(3,4,10, sharey=ax1)\n",
    "female_lowclass = df.Survived[df.Sex == 'female'][df.Pclass == 3].value_counts()\n",
    "female_lowclass.plot(kind='bar', label='female, low class', color='pink', alpha=a)\n",
    "ax5.set_xticklabels([\"Died\",\"Survived\"], rotation=0)\n",
    "ax5.set_xlim(-1, len(female_lowclass))\n",
    "plt.legend(loc='best')\n",
    "\n",
    "ax6 = fig.add_subplot(3,4,11, sharey=ax1)\n",
    "male_lowclass = df.Survived[df.Sex == 'male'][df.Pclass == 3].value_counts()\n",
    "male_lowclass.plot(kind='bar', label='male, low class',color='lightblue', alpha=a)\n",
    "ax6.set_xticklabels([\"Died\",\"Survived\"], rotation=0)\n",
    "ax6.set_xlim(-1, len(male_lowclass))\n",
    "plt.legend(loc='best')\n",
    "\n",
    "ax7 = fig.add_subplot(3,4,12, sharey=ax1)\n",
    "male_highclass = df.Survived[df.Sex == 'male'][df.Pclass != 3].value_counts()\n",
    "male_highclass.plot(kind='bar', label='male highclass', alpha=a, color='steelblue')\n",
    "ax7.set_xticklabels([\"Died\",\"Survived\"], rotation=0)\n",
    "ax7.set_xlim(-1, len(male_highclass))\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "# Chapter 4: Feature Engineering with Pandas\n",
    "> - Discretize continuous features\n",
    "> - Decompose features (e.g.: categorical, date/time, etc.)\n",
    "> - Add promising transformations of features (e.g.: log(x), sqrt(x), x^2, etc.)\n",
    "> - Aggregate features into promising new features\n",
    "> - Standardize or normalize features\n",
    "\n",
    "### Step 1: Dataset Import and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.copy() # Copy the df we have explored into train_df\n",
    "test_df = pd.read_csv('./dataset/test.csv') # Importing the testing dataset\n",
    "test_df = test_df.drop(['Ticket', 'Cabin'], axis = 1) # Dropping unimportant features\n",
    "test_df = test_df.dropna() # Removing NaN noisy data\n",
    "combine = [train_df, test_df] # Storing both datasets as a list for 'For' loop computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 2: Convert categorical features into discrete values (binary/m-nary) \n",
    "#### __(A) Sex feature__ ('Male': 0, 'Female': 1)\n",
    "```tex\n",
    "We will convert the sex categories, 'Male' and 'Female', with binary values. \n",
    "We perform the following mapping using one-hot encoding - Male: 0, Female: 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in combine: \n",
    "    dataset['Sex'] = dataset['Sex'].map({'male': 0, 'female': 1}).astype(int)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### __(B) Embarked feature__ ('S': 0, 'C': 1, 'Q': 2)\n",
    "```tex\n",
    "We will convert the embarked categories, 'S', 'C' and 'Q', with m-nary values. \n",
    "We perform the following mapping using one-hot encoding - S: 0, C: 1, Q: 2\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in combine:\n",
    "    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 3: Aggregate features into new promising features\n",
    "#### __(A) FamilySize feature__ (combining SibSp and Parch features)\n",
    "```tex\n",
    "We can create a new feature called 'FamilySize' which denotes the number of family members by combining two features, 'Parch' and 'SibSp'. We can append a new feature column called 'FamilySize' after computation.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in combine:\n",
    "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1 # to account for Python's zero-indexing\n",
    "\n",
    "# Quick glance of our new feature\n",
    "train_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### __(B) IsAlone feature__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in combine: \n",
    "    dataset['IsAlone'] = 0\n",
    "    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "\n",
    "train_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index = False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 4: Discretise continuous features\n",
    "#### __(A) AgeBand feature__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['AgeThresholds'] = pd.cut(train_df['Age'], 5) # Splice Age into 5 categories\n",
    "train_df[['AgeThresholds', 'Survived']].groupby(['AgeThresholds'], as_index=False).mean().sort_values(by='AgeThresholds', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```tex\n",
    "We can replace the continuous variable 'Age' with ordinals based on the computed 'AgeBand' above.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in combine:    \n",
    "    # Create a new column called 'AgeBand'\n",
    "    dataset['AgeBand'] = dataset['Age'].copy()\n",
    "    \n",
    "    # Overwrite the values based on the AgeThresholds computed\n",
    "    dataset.loc[ dataset['AgeBand'] <= 16, 'AgeBand'] = 0\n",
    "    dataset.loc[(dataset['AgeBand'] > 16) & (dataset['AgeBand'] <= 32), 'AgeBand'] = 1\n",
    "    dataset.loc[(dataset['AgeBand'] > 32) & (dataset['AgeBand'] <= 48), 'AgeBand'] = 2\n",
    "    dataset.loc[(dataset['AgeBand'] > 48) & (dataset['AgeBand'] <= 64), 'AgeBand'] = 3\n",
    "    dataset.loc[ dataset['AgeBand'] > 64, 'AgeBand']\n",
    "    \n",
    "    # Converting 'Age' and 'AgeBand' features into integers\n",
    "    dataset['Age'] = dataset['Age'].astype(int)\n",
    "    dataset['AgeBand'] = dataset['AgeBand'].astype(int)\n",
    "    \n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### __(B) FareBand feature__\n",
    "```tex\n",
    "We may also want to round off the fare to two decimal places as it represents currency and then calculate the threshold of 'FareBand' for categorization.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)\n",
    "train_df['FareThresholds'] = pd.qcut(train_df['Fare'], 4)\n",
    "train_df[['FareThresholds', 'Survived']].groupby(['FareThresholds'], as_index=False).mean().sort_values(by='FareThresholds', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```tex\n",
    "Convert the 'Fare' feature to ordinal values based on the 'FareBand'.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in combine:\n",
    "    # Create a new column called 'FareBand'\n",
    "    dataset['FareBand'] = dataset['Fare'].copy()\n",
    "    \n",
    "    # Overwrite the values based on the FareThresholds computed\n",
    "    dataset.loc[ dataset['FareBand'] <= 8.05, 'FareBand'] = 0\n",
    "    dataset.loc[(dataset['FareBand'] > 8.05) & (dataset['FareBand'] <= 15.646), 'FareBand'] = 1\n",
    "    dataset.loc[(dataset['FareBand'] > 15.646) & (dataset['FareBand'] <= 33.0), 'FareBand']   = 2\n",
    "    dataset.loc[ dataset['FareBand'] > 33.0, 'FareBand'] = 3\n",
    "    \n",
    "    # Converting 'Fare' and 'FareBand' features into integers\n",
    "    dataset['Fare'] = dataset['Fare'].astype(int)\n",
    "    dataset['FareBand'] = dataset['FareBand'].astype(int)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 4: Feature Selection\n",
    "```tex\n",
    "We can now proceed to drop features that provide no further useful information for the task. \n",
    "We can now remove the 'AgeThresholds' and 'FareThresholds' columns, as they are only needed to determine the threshold values for different ordinal types for Age and Fare. The 'Name' and 'PassengerId' columns can also be dropped to simplify the datasets. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(['AgeThresholds'], axis=1)\n",
    "train_df = train_df.drop(['FareThresholds'], axis=1)\n",
    "train_df = train_df.drop(['Name', 'PassengerId'], axis=1)\n",
    "test_df = test_df.drop(['Name', 'PassengerId'], axis=1)\n",
    "combine = [train_df, test_df]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 5: Overview of Pre-processed Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick reference of feature abbreviations\n",
    "|Feature|Definition|Key|\n",
    "|:--------- |:----------------|:------------- |\n",
    "|**survival**|Survival|0 = No, 1 = Yes|\n",
    "|**pclass**|Ticket class|1 = 1st, 2 = 2nd, 3 = 3rd|\n",
    "|**sex**|Sex|\n",
    "|**Age**|Age in years|\n",
    "|**sibsp**|# of siblings / spouses aboard the Titanic|\n",
    "|**parch**|# of parents / children aboard the Titanic|\n",
    "|**ticket**|Ticket number|\n",
    "|**fare**|Passenger fare|\n",
    "|**cabin**|Cabin number|\n",
    "|**embarked**|Port of Embarkation|C = Cherbourg, Q = Queenstown, S = Southampton|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Chapter 5: Machine Learning with Scikit-Learn\n",
    "\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/KenYew/YME-Python-Workshop/master/images/scikit-learn_logo.png\" width=\"400px\" align = \"right\"/>\n",
    "</div>\n",
    "\n",
    "#### Model, predict and solve\n",
    "In this chapter, we will be experimenting with various supervised learning models and evaluate their performances in the end. \n",
    "For this problem, our choices of machine learning models are: \n",
    "\n",
    "- Logistic Regression\n",
    "- k-Nearest Neighbors (k-NN)\n",
    "- Support Vector Machines\n",
    "- Naive Bayes Classifier\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- Adaptive Boosting Ensemble Method (AdaBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## Step 1: Preparing our training and testing datasets\n",
    "\n",
    "<div>\n",
    "<img src=\"https://s3.amazonaws.com/assets.datacamp.com/production/course_2906/datasets/dtree_test_set.png\" width=\"400px\" align = \"middle\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop('Survived', axis=1)\n",
    "y_train = train_df['Survived']\n",
    "X_test = test_df.copy()\n",
    "X_train.shape, y_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Train various machine learning models\n",
    "```tex\n",
    "We will be training various machine learning models and determine which machine learning algorithm yields the highest training and testing score accuracies. Training score accuracy is tells us how accurate the model is able to correctly predict the outcome based on learning from the traning dataset and likewise for the testing score accuracy. \n",
    "```\n",
    "\n",
    "### (A) Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(max_iter=1000)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "acc_logreg = round(logreg.score(X_train, y_train) * 100, 2)\n",
    "print('Training Accuracy Score: '+ str(acc_logreg) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Append our results__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = X_test.copy()\n",
    "results_df['logreg'] = np.array(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### (B) Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "y_pred = svc.predict(X_test)\n",
    "acc_svc = round(svc.score(X_train, y_train) * 100, 2)\n",
    "results_df['svc'] = np.array(y_pred)\n",
    "print('Training Accuracy Score: '+ str(acc_svc) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### (C) k-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "acc_knn = round(knn.score(X_train, y_train) * 100, 2)\n",
    "results_df['knn'] = np.array(y_pred)\n",
    "print('Training Accuracy Score: '+ str(acc_knn) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### (D) Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred = gnb.predict(X_test)\n",
    "acc_gnb = round(gnb.score(X_train, y_train) * 100, 2)\n",
    "results_df['gnb'] = np.array(y_pred)\n",
    "print('Training Accuracy Score: '+ str(acc_gnb) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### (E) Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(X_train, y_train)\n",
    "y_pred = dtc.predict(X_test)\n",
    "acc_dtc = round(dtc.score(X_train, y_train) * 100, 2)\n",
    "results_df['dtc'] = np.array(y_pred)\n",
    "print('Training Accuracy Score: '+ str(acc_dtc) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### (F) Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "rfc.fit(X_train, y_train)\n",
    "y_pred = rfc.predict(X_test)\n",
    "acc_rfc = round(rfc.score(X_train, y_train) * 100, 2)\n",
    "results_df['rfc'] = np.array(y_pred)\n",
    "print('Training Accuracy Score: '+ str(acc_rfc) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### (G) AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = AdaBoostClassifier()\n",
    "abc.fit(X_train, y_train)\n",
    "y_pred = abc.predict(X_test)\n",
    "acc_abc = round(abc.score(X_train, y_train) * 100, 2)\n",
    "results_df['abc'] = np.array(y_pred)\n",
    "print('Training Accuracy Score: '+ str(acc_abc) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Support Vector Machines', 'k-Nearest Neighbors', 'Naive Bayes', 'Decision Trees', 'Random Forest', 'AdaBoost'],\n",
    "    'Score': [acc_logreg, acc_svc, acc_knn, acc_gnb, acc_dtc, acc_rfc, acc_abc]})\n",
    "models.sort_values(by='Score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Resultant Dataframe with Machine Learning Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The next steps...\n",
    "### Ways to boost model score\n",
    "#### Hyper-parameter tuning\n",
    "> 1. Use GridSearchCV or RandomSearchCV methods from Scikit-Learn\n",
    "> 2. Apply a parameter grid containing possible hyperparameters to loop through\n",
    "> 3. Determine the optimal set of hyperparameters for the model to yield the highest accuracy score. \n",
    "#### Cross-validation techniques\n",
    "> 1. Split the dataset into k-number of folds\n",
    "> 2. In each k-iteration, the k-th fold of the dataset is selected as the testing data while the rest of the dataset is used as the training data\n",
    "> 3. This step is repeated going through all k-folds of the dataset as testing data\n",
    "> 4. The iteration with the selected k fold that yields the highest accuracy score will be chosen as the default training and testing dataset split when training our machine learning model. \n",
    "#### Try a different machine learning or deep learning model\n",
    "> - Depending on the nature of your dataset, you can always use different machine learning or deep learning algorithms that are more optimised for the problem. \n",
    "> - For example: Image recognition - Use Convolutional Neural Networks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
